---
title: "Krydsvalidering"
author: 'Aalborg Intelligence'
date: ''
format:
    html: 
      self-contained: true
      toc: true
      toc-title: Indhold
      toc-location: left
      related-formats-title: "Andre formater"
    pdf: default
reference-location: margin
editor_options: 
  chunk_output_type: console
crossref:
  fig-prefix: figur   # (default is "Figure")
  tbl-prefix: tabel    # (default is "Table")
  exm-prefix: eksempel
  eq-prefix: ''
  fig-title: Figur
  exm-title: Eksempel
  tbl-title: Tabel
label:
    fig: Figur
title-block-author-single: "Forfatter"
fig-cap-location: margin
---

```{r}
#| include=FALSE
set.seed(12345)
x<-(1:20)/8 # x-værdierne
x1<- -(x^2 -2* x +1) # det underliggende andengradspolynomium
x2<-1:250/100 # bruges i plot af grafer

sigma=0.3
eps<-rnorm(20,0,sigma) # error terms
y<-x1+eps # y-værdierne
```

Forudsætninger: lineær regression, mindste kvadraters metode, polynomier

## Polynomiel regression og modeludvælgelse 

Denne note handler om, hvad man kan gøre, når man har flere forskellige
modeller for data at vælge imellem og gerne vil vælge den bedste. Noten
introducerer først polynomiel regression, der bruges som gennemgående
eksempel. Mod slutningen diskuteres, hvordan de samme principper kan
bruges i forbindelse med nogle af de andre algoritmer, der er gennemgået
her på siden.

## Polynomiel regression

### Lineær regression

Fra gymnasieundervisningen kender I lineær regression. Lad os sige, at
vi har datapunkter $(x_i,y_i)$, hvor $i=1,2,\ldots,n$. Vi vil gerne
finde den rette linje, der bedst tilnærmer punkterne. I denne note
kalder vi linjens skæring for $a_0$ og hældningen for $a_1$. Linjen har
altså funktionsforskriften $$f(x) = a_0 + a_1x.$$ For at finde den
bedste linje til at beskrive vores data, søger vi de værdier $a_0$ og
$a_1$ som gør, at $a_0 + a_1x_i$ er så tæt på $y_i$ som muligt. Vi vil
altså gerne gøre afvigelserne fra linjen $y_i - (a_0 + a_1 x_i)$ så små
som muligt. Som et samlet mål for hvor store disse afvigelser er for
alle vores punkter, kigger vi på kvadratsummen af afvigelserne
$$E=\sum_{i=1}^n \left(y_i - (a_0 + a_1x_i) \right)^2.$$ Vi vælger så de
værdier $a_0$ og $a_1$, der gør $E$ mindst mulig. Dette kaldes *mindste
kvadraters metode*.

### Kvadratisk regression

Hvad nu hvis det slet ikke ligner, at der er en lineær sammenhæng når vi
tegner vores datapunkter ind i et koordinatsystem? Er det så overhovedet
en god idé at forsøge med en lineær regression? På nedenstående plot ser
det fx ikke ud til at punkterne følger en ret linje.
```{r}
#| echo: false
par(mfrow = c(1,2))
model<-lm(y ~ x)
plot(x,y)
plot(x,y)
lines(x2, predict(model,data.frame(x=x2)), col="red")
```

Når $x$ ligger mellem 0 og 1, kunne der godt se ud til at være en svagt
stigende tendens, mens der ser ud til at være en aftagende tendens for
$x>1.5$. Den rette linje ser heller ikke ud til at følge punkterne
særlig godt. Måske en parabel passer bedre på data?

```{r degree2}
#| echo: false
model<-lm(y ~ x + I(x^2))
plot(x,y)
lines(x2,predict(model,data.frame(x=x2)),col="red")
```

Det ser ud til, at parablen følger datapunkterne langt bedre. Vi kunne
således prøve at modellere $y$ ved hjælp af et andengradspolynomium i
$x$. Lad $f$ betegne andengradspolynomiet $$f(x) = a_0 + a_1x + a_2x^2$$
med koefficienter $a_0,a_1,a_2\in \mathbb{R}$.

Hvordan finder man så det andengradspolynomium, der bedst beskriver
datapunkterne? Tilgangen er faktisk den samme som den mindste kvadraters
metode I kender fra lineær regression. Vi søger de værdier
$a_0, a_1 ,a_2$ som gør, at $f(x_i)$ kommer så tæt på $y_i$ som muligt.
Vi vil altså gerne gøre forskellene $y_i - f(x_i)$ så små som muligt. Vi
kigger derfor på kvadratsummen af disse forskelle
$$E=\sum_{i=1}^n (y_i - f(x_i))^2 = \sum_{i=1}^n \left(y_i - (a_0 + a_1x_i + a_2x_i^2)\right)^2.$$
Vi søger så de værdier $a_0,a_1,a_2$, der minimerer $E$.

Gør man det i vores lille dataeksempel, fås netop den parabel, der er
tegnet ind i koordinatsystemet ovenfor. Vi ser, at den beskriver data
langt bedre end den rette linje. Eksemplet viser i øvrigt vigtigheden af
at tegne datapunkterne ind i et koordinatsystem inden man laver lineær
regression. Ellers kan man nemt komme til at overse en eventuel
ikke-lineær sammenhæng.

### Polynomiel regression generelt

Men hvordan kan vi nu vide at et andengradspolynomium er det bedste til
at beskrive data? Måske et polynomium af endnu højere grad ville være
bedre? Man kan tilpasse tredje- og højeregradspolynomier til data på en
helt tilsvarende måde. Vi kan fx prøve at tilpasse et
tredjegradspolynomium $$f(x) = a_0 + a_1x + a_2x^2 +a_3x^3.$$ Det bedste
tredjegradspolynomium er igen det, der minimerer kvadratsummen
$$E=\sum_{i=1}^n (y_i - f(x_i))^2 = \sum_{i=1}^n \left(y_i - (a_0 + a_1x_i + a_2x_i^2 + a_3x^3 )\right)^2.$$
Grafen for det bedste tredjegradspolynomium er indtegnet for vores
dataeksempel. Andengradspolynomiet er indtegnet med rød til
sammenligning.

```{r}
#| echo: false
model3<-lm(y ~ x + I(x^2) + I(x^3))
plot(x,y)
lines(x2,predict(model,data.frame(x=x2)),col="red")
lines(x2,predict(model3,data.frame(x=x2)),col="blue")
legend(x = "bottomleft",          # Position
       legend = c("Grad 2", "Grad 3"),  # Legend texts
       lty = c(1, 1),           # Line types
       col = c("red", "blue"),           # Line colors
       lwd = 2) 
```

Det er ikke så let at se forskel. De to polynomier ser ud til at passe
nogenlunde lige godt på vores data. Hvis vi zoomer ud på plottet
ovenfor, er der dog en klar forskel:

```{r extrapolate}
#| echo: false
model3<-lm(y ~ x + I(x^2) + I(x^3))
plot(x,y, xlim=c(-1,3),ylim=c(-4,0.5))
x2<-1:400/100 -1
lines(x2, predict(model, data.frame(x=x2)), col="red")
lines(x2, predict(model3, data.frame(x=x2)), col="blue")
legend(x = "bottomleft",          # Position
       legend = c("Grad 2", "Grad 3"),  # Legend texts
       lty = c(1, 1),           # Line types
       col = c("red", "blue"),           # Line colors
       lwd = 2) 
```

Selv om der ikke var stor forskel på anden- og tredjegradspolynomiet på
intervallet $[0;2,5]$ hvor alle $x$-værdierne i vores datasæt lå, så er
der stor forskel når vi kommer uden for dette interval. Man skal derfor
passe på med at drage konklusioner om $x$-værdier uden for intervallet
hvor $x$-værdierne i vores datasæt ligger (det kaldes at ekstrapolere),
da disse kan være meget følsomme over for, hvilken grad vi har valgt for
vores polynomium. I det hele taget er det en ulempe ved polynomiel
regression, at polynomierne har en tendens til at opføre sig vildt i
enderne.

## Overfitting

Det er altså svært at afgøre med det blotte øje, om anden- eller
tredjegradspolynomiet passer bedst til punkterne. Hvordan vælger vi så
hvad der er bedst? Som mål for, hvor tæt polynomiet er på data, kan vi
kigge på kvadratsummen $$E=\sum_{i=1}^n (y_i - f(x_i))^2.$$ For
andengradspolynomiet får vi en kvadratsum på $E=1,14$, mens vi får
$E=1,10$ for tredjegradspolynomiet. Tredjegradspolynomiet kommer altså
tættere på data end andengradspolynomiet. Det er på den anden side ikke
så overraskende, for ved at sætte $a_3=0$ i et tredjegradspolynomium fås
et andengradspolynomium. Andengradspolynomier er altså specialtilfælde
af tredjegradspolynomier. Vi vil derfor altid kunne tilpasse data mindst
lige så godt med et tredjegradspolynomium som med et
andengradspolynomium.

Kan det så altid betale sig at bruge et polynomium af højere grad? Lad
os prøve med et syvendegradspolynomium. Vi søger
$$f(x) = a_0 + a_1x + a_2x^2 +a_3x^3 + a_4x^4 + a_5x^5 +a_6x^6 +a_7x^7,$$
der minimerer kvadratsummen $$E=\sum_{i=1}^n (y_i - f(x_i))^2 .$$ Det
bedste syvendegradspolynomium i vores lille dataeksempel er indtegnet
med blå nedenfor:

```{r degree7}
#| echo: false
model7<-lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7))
plot(x,y)
lines(x2,predict(model,data.frame(x=x2)),col="red")
lines(x2,predict(model7,data.frame(x=x2)),col="blue")
legend(x = "bottomleft",          # Position
       legend = c("Grad 2", "Grad 7"),  # Legend texts
       lty = c(1, 1),           # Line types
       col = c("red", "blue"),           # Line colors
       lwd = 2) 
```

Kvadratsummen er på kun $E=0.90$, så umiddelbart virker det til at være
en meget bedre model. Der er dog visse problemer. Det ses, at den grafen
bugter sig meget for at komme så tæt som muligt på datapunkterne. Dels
virker det urealistisk, at den faktiske sammenhæng mellem $x$ og $y$
skulle være så kompliceret. Dels opstår der et problem, hvis vi kommer
med nye datapunkter. Her er polynomierne fra før tegnet sammen med 20
nye datapunkter i grøn. Nu beskriver syvendegradspolynomiet pludselig
ikke datapunkterne så godt længere.

```{r overfitting}
#| echo: false
par(mfrow = c(1,2))
set.seed(1234)
v<-rnorm(20,0,sigma)
#plot(x,y)
x2<-1:400/100 -1
plot(x,x1+v,col="green", main="Grad 2")
lines(x2,predict(model,data.frame(x=x2)),col="red")
#plot(x,y)
plot(x,x1+v,col="green",main="Grad 7")
lines(x2,predict(model7,data.frame(x=x2)),col="blue")

```

Det der sker her er et eksempel på det fænomen, der kaldes
*overfitting*: syvendegradspolynomiet havde tilpasset sig for godt til
lige netop de sorte datapunkter. Når graden bliver for høj, begynder
polynomiet at tilpasse sig nogle strukturer i data, som i virkeligheden
bare skyldes tilfældigheder. Det fungerer rigtig godt til at beskrive
det oprindelige data, men til gengæld er det dårligt til at forudsige
nye dataværdier.

Jo højere grad man vælger at polynomiet skal have, desto bedre kan man
tilnærme data. Med $n$ datapunkter, kan man faktisk altid finde et
polynomium af grad $n-1$, der går igennem alle datapunkterne, men det er
klart, at nye datapunkter ikke nødvendigvis følger dette polynomium
særlig godt.

### Modelfleksibilitet

Det vi så ovenfor var, at vi havde forskellige modeller for data
(polynomier af forskellig grad). Modellerne havde forskellig
*fleksibilitet* (høj grad gav høj fleksibilitet). Når vi brugte en model
med for lav fleksibilitet (lineær regression) kunne vi ikke tilpasse
modellen godt nok til data. Når vi valgte en model med for høj
fleksibilitet (polynomium af grad syv) opstod der problemer med
overfitting og modellen var ikke god til at beskrive nye data.

Det tilsvarende problem opstår også i andre sammenhænge, når man har
flere forskellige modeller at vælge imellem. Nogle vil være for
ufleksible til at beskrive data ordentligt. Andre vil være for fleksible
og føre til overfitting. Så hvordan finder vi et godt kompromis? Det
handler de næste to kapitler om.

## Trænings- og testdata

Når vi har et datasæt og prøver at tilpasse en polynomiel
regressionsmodel, siger vi, at vi *træner* modellen. Datasættet vi
bruger til at træne modellen kaldes *træningsdata*. Som vi så ovenfor,
indebærer det en risiko for overfitting når vi træner modellen. Hvis vi
kommer med et nyt datasæt af samme type, passer modellen ikke
nødvendigvis særlig godt.

For at vurdere hvilken grad af polynomiet der passer bedst, kan vi se
på, hvilken model der er bedst til at forudsige (også kaldet
*prædiktere*) $y$-værdierne i et nyt datasæt. Det nye datasæt kaldes
*testdata*. Lad os kalde testdatapunkterne for
$(x_i^{test},y^{test}_i)$, hvor $i=1,\ldots,m$. Man kan måle, hvor godt
modellen forudsiger testdata ved at se på forskellene
$y_i^{test}-f(x_i^{test})$ mellem de observerede værdier $y_i^{test}$ og
dem der forudsiges af polynomiet $f(x_i^{test})$. Som samlet mål for,
hvor godt modellen forudsiger testdata, beregner vi kvadratsummen af
disse forskelle
$$E^{test} = \sum_{i=1}^{m} \left(y_i^{test} - {f}\left(x_i^{test}\right)\right)^2.$$
Man kalder $E^{test}$ for *tabsfunktionen*.

I praksis har man typisk kun et datasæt til rådighed, og man er derfor
nødt til først at dele data i to. Hele processen med at inddele data og
først træne modellen og derefter teste den er som følger:

-   Vælg en polynomiumsgrad $p$.

-   Datasættet inddeles i to dele, en der bruges som træningsdata og en
    der bruges som testdata. Vi vil betegne punkterne i træningsdata med
    $(x_i^{træn},y_i^{træn})$, $i=1,\ldots,n$, og punkterne i testdata
    med $(x_i^{test},y_i^{test})$, $i=1,\ldots,m$.

-   Vi træner modellen på træningsdatasættet og finder det
    $p$te-gradspolynomium $$f(x)=a_0 + a_1 x + \dotsm + a_px^p$$ der
    passer bedst på data. Mindste kvadraters metode benyttes til at
    bestemme $a_0,\ldots,a_p$ som de tal, der minimerer
    $$E^{træn}(p)=\sum_{i=1}^{n} (y_i^{træn} - {f}(x_i^{træn}))^2.$$ Det
    bedste polynomium kalder vi $\hat{f}$.

-   Når vi har valgt funktionen $\hat{f}$ på baggrund af træningsdataet,
    tester vi den på testdataet ved at beregne
    $$E^{test}(p)=\sum_{i=1}^{m} (y_i^{test} - \hat{f}(x_i^{test}))^2.$$
    Jo mindre $E^{test}(p)$ er, des bedre passer $\hat{f}$ på testdata.

Denne procedure kan gentages for forskellige værdier af
polynomiumsgraden $p$. Det $p$, der giver den mindste værdi af
$E^{test}(p)$ svarer altså til den model, der er bedst til at forudsige
værdierne i vores testdata, og vi vælger derfor at bruge dette $p$.

I vores eksempel ovenfor får vi $E^{test}(2)=1,82$ og $E^{test}(7)=2,29$
når vi bruger de grønne datapunkter som testdata. Andengradspolynomiet
er altså bedre end syvendegradspolynomiet til at forudsige testdata.
Bemærk at i begge tilfælde er $E^{test}$ langt større end $E^{træn}$
fordi modellen kun er tilpasset til træningsdata.

## Krydsvalidering

Der er et problem med tilgangen ovenfor. Når man træner en model, er det
altid en fordel at have så meget data som muligt, da man så har mulighed
for at træne modellen meget præcist. Problemet er, at hvis man bruger
det meste af data som træningsdata, er der ikke meget tilbage til at
teste på, og vi risikerer overfitting.

Krydsvalidering løser dette problem på snedig vis ved at gentage
trænings- og testproceduren flere gange. For at lave $k$-fold
krydsvalidering deler man data op i $k$ lige store dele. I første fold
træner man modellen på alt data undtagen den første del og bruger første
del som testdata. Det given en tabsfunktion $E_1(p)$. Dette gentages så
$k$ gange, hvor man i den $i$te fold bruger den $i$te del af data som
testdata og resten som træningsdata og får en tabsfunktion $E_i(p)$.

Som et samlet mål for, hvor god modellen er, bruges summen af
tabsfunktionerne fra de $k$ fold
$$E(p)=E_1(p) + E_2(p) + \dotsm + E_k(p).$$ Man vælger så den model der
giver den mindste værdi af $E(p)$.

Fordelen ved krydsvalidering er, at man i hver fold bruger det meste af
data til at træne modellen på. Samtidig bliver hvert datapunkt alt i alt
brugt præcis en gang til at teste på. På den måde får man udnyttet data
bedre end hvis man bare laver en enkelt opdeling af data. Typisk vælger
man $k=5$ eller $k=10$.

```{tikz k-fold}
#| echo: false
\begin{tikzpicture}
	\draw[fill,red!30] (0,0) -- (2,0) -- (2,0.5) -- (0,0.5) -- (0,0);
	\draw[fill, blue!30] (2,0) -- (10,0) -- (10,0.5) -- (2,0.5) -- (2,0);
	\node at (1,0.25) {test} ;
	\node at (6,0.25) {træning} ;
	\node at (-1,0.25) {Fold 1} ;
	
	\draw[fill,blue!30] (0,-1) -- (2,-1) -- (2,-0.5) -- (0,-0.5) -- (0,-1);
	\draw[fill, red!30] (2,-1) -- (4,-1) -- (4,-0.5) -- (2,-0.5) -- (2,-1);
	\draw[fill, blue!30] (4,-1) -- (10,-1) -- (10,-0.5) -- (4,-0.5) -- (4,-1);
	\node at (3,-0.75) {test} ;
	\node at (7,-0.75) {træning} ;
	\node at (-1,-0.75) {Fold 2} ;
	
	\draw[fill,blue!30] (0,-2) -- (4,-2) -- (4,-1.5) -- (0,-1.5) -- (0,-2);
	\draw[fill, red!30] (4,-2) -- (6,-2) -- (6,-1.5) -- (4,-1.5) -- (4,-2);
	\draw[fill, blue!30] (6,-2) -- (10,-2) -- (10,-1.5) -- (6,-1.5) -- (6,-2);
	\node at (5,-1.75) {test} ;
	\node at (8,-1.75) {træning} ;
	\node at (-1,-1.75) {Fold 3} ;
	
	\draw[fill,blue!30] (0,-3) -- (6,-3) -- (6,-2.5) -- (0,-2.5) -- (0,-3);
	\draw[fill, red!30] (6,-3) -- (8,-3) -- (8,-2.5) -- (6,-2.5) -- (6,-3);
	\draw[fill, blue!30] (8,-3) -- (10,-3) -- (10,-2.5) -- (8,-2.5) -- (8,-3);
	\node at (7,-2.75) {test} ;
	\node at (3,-2.75) {træning} ;
	\node at (-1,-2.75) {Fold 4} ;
	
	\draw[fill,blue!30] (0,-4) -- (8,-4) -- (8,-3.5) -- (0,-3.5) -- (0,-4);
	\draw[fill, red!30] (8,-4) -- (10,-4) -- (10,-3.5) -- (8,-3.5) -- (8,-4);
	\node at (9,-3.75) {test} ;
	\node at (4,-3.75) {træning} ;
	\node at (-1,-3.75) {Fold 5} ;
\end{tikzpicture}
```


### Krydsvalidering i andre sammenhænge

Krydsvalidering kan bruges i et væld af andre sammenhænge, hvor der skal
vælges mellem flere forskellige prædiktionsmodeller.

Hvis det der skal prædikteres er en talværdi, kan man gøre som ovenfor.
Algoritmen trænes på træningsdataet og bruges derefter til lave
prædiktioner $\hat{y}_i^{test}$, $i=1,\ldots ,m$, af værdierne i
testdatasættet. Disse sammenlignes med de faktiske værdier $y_i^{test}$
ved at se på forskellene $y_i^{test} - \hat{y}_i^{test}$ og beregne
tabsfunktionen
$$E^{test} = \sum_{i=1}^m(y_i^{test}-\hat{y}_i^{test})^2.$$ Modellen med
lavest $E^{test}$ er bedst til at lave nye prædiktioner.

Hvis der derimod er tale om et klassifikationsproblem, hvor der skal
prædikteres en klasse, skal man definere tabsfunktionen lidt anderledes.
Som før trænes algoritmen på træningsdataet og derefter bruges den til
lave prædiktioner af klasserne $\hat{y}_i^{test}$, $i=1,\ldots ,m$, i
testdatasættet. Disse sammenholdes med de faktiske klasser, og vi bruger
så *fejlraten* som tabsfuntion. Fejlraten angiver andelen af
observationerne i testdataet, der bliver klassificeret forkert, dvs.
$$
E^{test} =  \frac{1}{m}\cdot (\text{antal fejlklassifikationer i testdata}).
$$ {#eq-fejl}
Modellen med lavest $E^{test}$ har færrest fejlklassifikationer og
vælges derfor som den bedste.

Eksempler her fra siden, hvor krydsvalidering kan benyttes:

-   I forløbet \"Hvem ligner du mest\" sammenligner man et gråt punkt
    med alle punkter inden for en radius $r$ for at forudsige farven.
    Det er ikke oplagt, hvordan man skal vælge dette $r$. En mulighed er
    at komme med nogle gode bud $r_1,\ldots, r_N$ på radier og så bruge
    krydsvalidering til at vælge den bedste. En mulighed er at vælge
    $k=n$. Så får man det, der kaldes *leave-one-out* krydsvalidering. I
    hver fold bliver der så kun et datapunkt i testdata, mens resten
    bruges som træningsdata. Det ene testdatapunkt farves gråt, og
    farven prædikteres ud fra de øvrige datapunkter der ligger inden for
    den valgte radius. Prædiktionen sammenlignes med punktets rigtige
    farve. Dette gentages for alle datapunkter og fejlraten (@eq-fejl)
    beregnes til sidst. Vi vælger så den radius, der har lavest fejlrate.
    
    
```{r unused-code}
#| include: false
## Kode der ikke bruges lige nu

# RSS test
#RSS2<-sum((x1+v -predict(model,data.frame(x=x)))^2)
#RSS3<-sum((x1+v -predict(model3,data.frame(x=x)))^2)
#RSS7<-sum((x1+v -predict(model7,data.frame(x=x)))^2)
#RSS træn
#RSS2<-sum((y-predict(model,data.frame(x=x)))^2)
#RSS3<-sum((y-predict(model3,data.frame(x=x)))^2)
#RSS7<-sum((y -predict(model7,data.frame(x=x)))^2)

# par(mfrow = c(1,2))
# model15<-lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7)
#             +I(x^8) + I(x^9) + I(x^10)+ I(x^11) + I(x^12))
# plot(x,y)
# lines(x2,predict(model,data.frame(x=x2)),col="red")
# lines(x2,predict(model15,data.frame(x=x2)),col="blue")
# plot(x,x1+v,col="green")
# lines(x2,predict(model,data.frame(x=x2)),col="red")
# lines(x2,predict(model15,data.frame(x=x2)),col="blue")

```

