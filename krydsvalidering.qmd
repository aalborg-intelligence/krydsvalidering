---
title: "Polynomiel regression og modeludvælgelse "
author: 'Aalborg Intelligence'
image: "images/overfitting.png"
date: ''
format:
    html: 
      self-contained: true
      toc: true
      toc-title: Indhold
      toc-location: left
      related-formats-title: "Andre formater"
    pdf: default
reference-location: margin
editor_options: 
  chunk_output_type: console
crossref:
  fig-prefix: figur   # (default is "Figure")
  tbl-prefix: tabel    # (default is "Table")
  exm-prefix: eksempel
  eq-prefix: ''
  fig-title: Figur
  exm-title: Eksempel
  tbl-title: Tabel
label:
    fig: Figur
title-block-author-single: "Forfatter"
fig-cap-location: margin
---


```{r}
#| include=FALSE
set.seed(12345)
x<-(1:20)/8 # x-værdierne
x1<- -(x^2 -2* x +1) # det underliggende andengradspolynomium
x2<-1:250/100 # bruges i plot af grafer

sigma=0.3
eps<-rnorm(20,0,sigma) # error terms
y<-x1+eps # y-værdierne
```



Denne note handler om, hvad man kan gøre, når man har flere forskellige
modeller for data at vælge imellem og gerne vil vælge den bedste. Noten
introducerer først polynomiel regression, der bruges som gennemgående
eksempel. Mod slutningen diskuteres, hvordan de samme principper kan
bruges i forbindelse med nogle af de andre algoritmer, der er gennemgået
her på siden.

## Polynomiel regression

### Lineær regression

Fra gymnasieundervisningen kender I lineær regression. Lad os sige, at
vi har datapunkter $(x_i,y_i)$, hvor $i=1,2,\ldots,n$. Vi vil gerne
finde den rette linje, der bedst beskriver punkterne. I denne note
kalder vi linjens skæring for $a_0$ og hældningen for $a_1$. Linjen har
altså funktionsforskriften[^1] 

$$f(x) = a_0 + a_1x.$$ 

For at finde den bedste linje til at beskrive vores data, søger vi de værdier $a_0$ og
$a_1$, som gør, at $a_0 + a_1x_i$ er så tæt på $y_i$ som muligt. Vi vil
altså gerne gøre afvigelserne fra linjen $y_i - (a_0 + a_1 x_i)$ så små
som muligt. Bemærk, at disse afvigelser svarer til residualerne

$$
r_i=y_i - (a_0 + a_1 x_i).
$$

Som et samlet mål for hvor store disse afvigelser er for
alle vores punkter, kigger vi på kvadratsummen af afvigelserne/residualerne

$$
\begin{aligned}
E &= \left(y_1 - (a_0 + a_1 x_1) \right)^2 + \left(y_2 - (a_0 + a_1 x_2) \right)^2 + \cdots + \left(y_n - (a_0 + a_1 x_n) \right)^2 \\
& = r_1^2 + r_2^2 + \cdots + r_n^2
\end{aligned}
$$

Nu er det lidt omstændeligt at skrive summen ud, som vi har gjort det ovenfor. I matematik vil man ofte skrive en sådan sum lidt mere kompakt ved hjælp af et summationstegn. Gør vi det, ser det sådan her ud:

$$
\begin{aligned}
E &=\sum_{i=1}^n \left(y_i - (a_0 + a_1x_i) \right)^2 \\ 
&= \sum_{i=1}^n r_i^2 .
\end{aligned}
$$

Vi vælger så de værdier $a_0$ og $a_1$, der gør $E$ mindst mulig. Dette kaldes *mindste
kvadraters metode*.

### Kvadratisk regression

Hvad nu hvis det slet ikke ligner, at der er en lineær sammenhæng, når vi
tegner vores datapunkter ind i et koordinatsystem? Er det så overhovedet
en god idé at forsøge med en lineær regression? På @fig-ret_linje ser
det for eksempel ikke ud til at punkterne følger en ret linje.

```{r fig.width=8, fig.height=4}
#| echo: false
#| fig-cap: Til venstre ses et punktplot af et datasæt. Til højre er den bedste rette linje indtegnet.
#| label: fig-ret_linje
par(mfrow = c(1,2),mar=c(4,4,1,1))
model<-lm(y ~ x)
plot(x,y,pch=16)
plot(x,y,pch=16)
lines(x2, predict(model,data.frame(x=x2)), col="magenta")
```

Det fremgår også af residualplottet i @fig-residualplot. Her kan vi tydeligt se, at der er et mønster i den måde, residualerne fordeler sig omkring $x$-aksen. Det er altså tegn på, at en ret linje ikke er velegnet til at beskrive datapunkterne.

```{r fig.width=4, fig.height=4}
#| echo: false
#| fig-cap: Residualplottet for den bedste rette linje indtegnet i @fig-ret_linje.
#| label: fig-residualplot
par(mar=c(4,4,1,1))
model<-lm(y ~ x)
yhat <- predict(model,data.frame(x=x))
plot(x,y-yhat,ylab="residual",pch=16)
abline(h=0)
```

Når $x$ ligger mellem 0 og 1, kunne der godt se ud til at være en svagt
stigende tendens i @fig-ret_linje, mens der ser ud til at være en aftagende tendens for
$x>1.5$. Det svarer til, at residualerne i @fig-residualplot først er negative, så positive og dernæst negative igen. Den rette linje i @fig-ret_linje ser heller ikke ud til at følge punkterne særlig godt. Måske en parabel passer bedre på data? 

```{r degree2, fig.width=4, fig.height=4}
#| echo: false
#| fig-cap: Datasættet fra @fig-ret_linje, men nu med en parabel indtegnet.
#| label: fig-parabel
par(mar=c(4,4,1,1))
model<-lm(y ~ x + I(x^2))
plot(x,y,pch=16)
lines(x2,predict(model,data.frame(x=x2)),col="red")
```

Det ser ud til, at parablen i @fig-parabel følger datapunkterne langt bedre. Vi kunne
således prøve at modellere $y$ ved hjælp af et andengradspolynomium i
$x$. Lad $f$ betegne andengradspolynomiet[^2] 

$$
f(x) = a_0 + a_1x + a_2x^2
$$

med koefficienter $a_0,a_1,a_2\in \mathbb{R}$.

Hvordan finder man så det andengradspolynomium, der bedst beskriver
datapunkterne? Tilgangen er faktisk den samme som den mindste kvadraters
metode, I kender fra lineær regression. Vi søger de værdier
$a_0, a_1$ og $a_2$, som gør, at $f(x_i)$ kommer så tæt på $y_i$ som muligt.
Vi vil altså gerne gøre forskellene $y_i - f(x_i)$ så små som muligt. Vi
kigger derfor på kvadratsummen af disse forskelle
$$E=\sum_{i=1}^n (y_i - f(x_i))^2 = \sum_{i=1}^n \left(y_i - (a_0 + a_1x_i + a_2x_i^2)\right)^2.$$
Vi søger så de værdier $a_0,a_1$ og $a_2$, der minimerer $E$.

Gør man det i vores lille dataeksempel, fås netop den parabel, der er
tegnet ind i koordinatsystemet i @fig-parabel. Vi ser, at den beskriver data
langt bedre end den rette linje. 

Eksemplet viser vigtigheden af
at tegne datapunkterne ind i et koordinatsystem, inden man laver lineær
regression. Ellers kan man nemt komme til at overse en eventuel
ikke-lineær sammenhæng.

### Polynomiel regression generelt

Men hvordan kan vi nu vide, at et andengradspolynomium er det bedste til
at beskrive data? Måske et polynomium af endnu højere grad ville være
bedre? Man kan tilpasse tredje- og højeregradspolynomier til data på en
helt tilsvarende måde. Vi kan for eksempel prøve at tilpasse et
tredjegradspolynomium 

$$
f(x) = a_0 + a_1x + a_2x^2 +a_3x^3.
$$ 

Det bedste tredjegradspolynomium er igen det, der minimerer kvadratsummen
$$E=\sum_{i=1}^n (y_i - f(x_i))^2 = \sum_{i=1}^n \left(y_i - (a_0 + a_1x_i + a_2x_i^2 + a_3x^3 )\right)^2.$$
Grafen for det bedste tredjegradspolynomium er indtegnet med grøn for vores
dataeksempel i @fig-2grad_3grad. Andengradspolynomiet er indtegnet med rød til
sammenligning.

```{r fig.height=4, fig.width=4}
#| echo: false
#| fig-cap: Et andengradspolynomium (rød) og et tredjegradspolynomium (grøn) fittet til data. 
#| label: fig-2grad_3grad
par(mar=c(4,4,1,1))
model3<-lm(y ~ x + I(x^2) + I(x^3))
plot(x,y,pch=16)
lines(x2,predict(model,data.frame(x=x2)),col="red")
lines(x2,predict(model3,data.frame(x=x2)),col="green")
legend(x = "bottomleft",          # Position
       legend = c("Grad 2", "Grad 3"),  # Legend texts
       lty = c(1, 1),           # Line types
       col = c("red", "green"),           # Line colors
       lwd = 2) 
```

Det er ikke så let at se forskel. De to polynomier ser ud til at passe
nogenlunde lige godt på vores data. Men i @fig-2grad_3grad_zoom har vi zoomet ud på figuren
ovenfor, og her er der en klar forskel:

```{r extrapolate, fig.width=4, fig.height=4}
#| echo: false
#| fig-cap: Plottet fra @fig-2grad_3grad, men hvor der nu er zoomet lidt ud. Her ses det tydeligt, at der i "enderne" bliver stor forskel på anden- og tredjegradspolynomiet.
#| label: fig-2grad_3grad_zoom
par(mar=c(4,4,1,1))
model3<-lm(y ~ x + I(x^2) + I(x^3))
plot(x,y, xlim=c(-2,3),ylim=c(-4,0.5),pch=16)
x2<-1:500/100 -2
lines(x2, predict(model, data.frame(x=x2)), col="red")
lines(x2, predict(model3, data.frame(x=x2)), col="green")
legend(x = "bottomleft",          # Position
       legend = c("Grad 2", "Grad 3"),  # Legend texts
       lty = c(1, 1),           # Line types
       col = c("red", "green"),           # Line colors
       lwd = 2) 
```

Selv om der ikke var stor forskel på anden- og tredjegradspolynomiet på
intervallet $[0;2,5]$ hvor alle $x$-værdierne i vores datasæt lå, så er
der stor forskel, når vi kommer uden for dette interval. Man skal derfor
passe på med at drage konklusioner om $x$-værdier uden for intervallet,
hvor $x$-værdierne i vores datasæt ligger (det kaldes at ekstrapolere),
da disse kan være meget følsomme over for, hvilken grad vi har valgt for
vores polynomium. I det hele taget er det en ulempe ved polynomiel
regression, at polynomierne har en tendens til at opføre sig vildt i
enderne.

## Overfitting

Det er altså svært at afgøre med det blotte øje, om anden- eller
tredjegradspolynomiet passer bedst til punkterne. Hvordan vælger vi så,
hvad der er bedst? Som mål for, hvor tæt polynomiet er på data, kan vi
kigge på kvadratsummen af afvigelserne $y_i - f(x_i)$, altså
$$E=\sum_{i=1}^n (y_i - f(x_i))^2.$$
For
andengradspolynomiet får vi en kvadratsum på $E=1.14$, mens vi får
$E=1.10$ for tredjegradspolynomiet. Tredjegradspolynomiet kommer altså
tættere på data end andengradspolynomiet. Det er på den anden side ikke
så overraskende, for ved at sætte $a_3=0$ i et tredjegradspolynomium fås
et andengradspolynomium. Andengradspolynomier er altså specialtilfælde
af tredjegradspolynomier. Vi vil derfor altid kunne tilpasse data mindst
lige så godt med et tredjegradspolynomium som med et
andengradspolynomium.

Kan det så altid betale sig at bruge et polynomium af højere grad? Lad
os prøve med et syvendegradspolynomium. Vi søger
$$f(x) = a_0 + a_1x + a_2x^2 +a_3x^3 + a_4x^4 + a_5x^5 +a_6x^6 +a_7x^7,$$
der minimerer kvadratsummen $$E=\sum_{i=1}^n (y_i - f(x_i))^2 .$$ Det
bedste syvendegradspolynomium i vores lille dataeksempel er indtegnet
med blå på @fig-2grad_7grad nedenfor:

```{r degree7, fig.width=4, fig.height=4}
#| echo: false
#| fig-cap: Et andengradspolynomium (rød) og et syvendegradspolynomium (blå) fittet til data. 
#| label: fig-2grad_7grad
par(mar=c(4,4,1,1))
model7<-lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7))
plot(x,y,pch=16)
lines(x2,predict(model,data.frame(x=x2)),col="red")
lines(x2,predict(model7,data.frame(x=x2)),col="blue")
legend(x = "bottomleft",          # Position
       legend = c("Grad 2", "Grad 7"),  # Legend texts
       lty = c(1, 1),           # Line types
       col = c("red", "blue"),           # Line colors
       lwd = 2) 
```

Kvadratsummen er på kun $E=0.90$, så umiddelbart virker det til at være
en meget bedre model. Der er dog visse problemer. Det ses, at grafen
bugter sig meget for at komme så tæt som muligt på datapunkterne. Dels
virker det urealistisk, at den faktiske sammenhæng mellem $x$ og $y$
skulle være så kompliceret. Dels opstår der et problem, hvis vi kommer
med nye datapunkter. I @fig-2grad_7grad_nypunkter er polynomierne fra før tegnet sammen med 20
nye datapunkter i orange. Nu beskriver syvendegradspolynomiet pludselig
ikke datapunkterne så godt længere.

```{r overfitting, fig.width=8, fig.height=4}
#| echo: false
#| fig-cap: Et andengradspolynomium (rød) og et syvendegradspolynomium (blå) fittet til 20 nye datapunkter (orange). 
#| label: fig-2grad_7grad_nypunkter
par(mfrow = c(1,2),mar=c(4,4,2,1))
set.seed(1234)
v<-rnorm(20,0,sigma)
#plot(x,y)
x2<-1:400/100 -1

plot(x,x1+v,col="orange", main="Grad 2", ,ylab="y",pch=16)
lines(x2,predict(model,data.frame(x=x2)),col="red")
#plot(x,y)
plot(x,x1+v,col="orange",main="Grad 7", ,ylab="y",pch=16)

#plot(x,x1+v,col="magenta", main="Grad 2",ylab="y",pch=16)
#lines(x2,predict(model,data.frame(x=x2)),col="red")
#plot(x,y)
#plot(x,x1+v,col="magenta",main="Grad 7",ylab="y",pch=16)

lines(x2,predict(model7,data.frame(x=x2)),col="blue")

```

Det, der sker her, er et eksempel på det fænomen, der kaldes
*overfitting*: syvendegradspolynomiet havde tilpasset sig for godt til
lige netop de sorte datapunkter. Når graden bliver for høj, begynder
polynomiet at tilpasse sig nogle strukturer i data, som i virkeligheden
bare skyldes tilfældigheder. Det fungerer rigtig godt til at beskrive
det oprindelige data, men til gengæld er det dårligt til at forudsige
nye dataværdier.

Jo højere grad man vælger, at polynomiet skal have, desto bedre kan man
tilnærme data. Med $n$ datapunkter, kan man faktisk altid finde et
polynomium af grad $n-1$, der går igennem alle datapunkterne, men det er
klart, at nye datapunkter ikke nødvendigvis følger dette polynomium
særlig godt.

### Modelfleksibilitet

Det, vi så ovenfor, var, at vi havde forskellige modeller for data
(polynomier af forskellig grad). Modellerne havde forskellig
*fleksibilitet* (høj grad gjorde polynomiet meget fleksibelt). Når vi brugte en model
med for lav fleksibilitet (lineær regression), kunne vi ikke tilpasse
modellen godt nok til data. Når vi valgte en model med for høj
fleksibilitet (polynomium af grad syv), opstod der problemer med
overfitting, og modellen var ikke god til at beskrive nye data.

Det tilsvarende problem opstår også i andre sammenhænge, når man har
flere forskellige modeller at vælge imellem. Nogle vil være for
ufleksible til at beskrive data ordentligt. Andre vil være for fleksible
og føre til overfitting. Så hvordan finder vi et godt kompromis? Det
handler det følgende om.

## Trænings- og testdata

Når vi har et datasæt og prøver at tilpasse en polynomiel
regressionsmodel, siger vi, at vi *træner* modellen. Datasættet vi
bruger til at træne modellen kaldes *træningsdata*. Som vi så ovenfor,
indebærer det en risiko for overfitting, når vi træner modellen. Hvis vi
kommer med et nyt datasæt af samme type, passer modellen ikke
nødvendigvis særlig godt.

For at vurdere hvilken grad af polynomiet der passer bedst, kan vi se
på, hvilken model der er bedst til at forudsige (også kaldet
*prædiktere*) $y$-værdierne i et nyt datasæt. Det nye datasæt kaldes
*testdata*. Lad os kalde testdatapunkterne for
$(x_i^{test},y^{test}_i)$, hvor $i=1,\ldots,m$. Man kan måle, hvor godt
modellen forudsiger testdata ved at se på forskellene
$y_i^{test}-f(x_i^{test})$ mellem de observerede værdier $y_i^{test}$ og
dem, der forudsiges af polynomiet $f(x_i^{test})$. Som samlet mål for,
hvor godt modellen forudsiger testdata, beregner vi kvadratsummen af
disse forskelle
$$E^{test} = \sum_{i=1}^{m} \left(y_i^{test} - {f}\left(x_i^{test}\right)\right)^2.$$
Man kalder $E^{test}$ for *tabsfunktionen*.

I praksis har man typisk kun et datasæt til rådighed, og man er derfor
nødt til først at dele data i to. Hele processen med at inddele data og
først træne modellen og derefter teste den er, som følger:

-   Vælg en polynomiumsgrad $p$.

-   Datasættet inddeles i to dele, én del der bruges som træningsdata, og én del
    der bruges som testdata. Vi vil betegne punkterne i træningsdata med
    $(x_i^{træn},y_i^{træn})$, $i=1,\ldots,n$, og punkterne i testdata
    med $(x_i^{test},y_i^{test})$, $i=1,\ldots,m$.

-   Vi træner modellen på træningsdatasættet og finder det
    $p$'te-gradspolynomium $$f(x)=a_0 + a_1 x + \dotsm + a_px^p$$ der
    passer bedst på data. Mindste kvadraters metode benyttes til at
    bestemme $a_0,\ldots,a_p$ som de tal, der minimerer
    $$E^{træn}(p)=\sum_{i=1}^{n} (y_i^{træn} - {f}(x_i^{træn}))^2.$$ Det
    bedste polynomium kalder vi $\hat{f}$.

-   Når vi har valgt funktionen $\hat{f}$ på baggrund af træningsdataet,
    tester vi den på testdataet ved at beregne
    $$E^{test}(p)=\sum_{i=1}^{m} (y_i^{test} - \hat{f}(x_i^{test}))^2.$$
    Jo mindre $E^{test}(p)$ er, des bedre passer $\hat{f}$ på testdata.

Denne procedure kan gentages for forskellige værdier af
polynomiumsgraden $p$. Det $p$, der giver den mindste værdi af
$E^{test}(p)$ svarer altså til den model, der er bedst til at forudsige
værdierne i vores testdata, og vi vælger derfor at bruge dette $p$.

I vores eksempel ovenfor får vi $E^{test}(2)=1.82$ og $E^{test}(7)=2.29$,
når vi bruger de orange datapunkter som testdata. Andengradspolynomiet
er altså bedre end syvendegradspolynomiet til at forudsige testdata.
Bemærk, at i begge tilfælde er $E^{test}$ langt større end $E^{træn}$,
fordi modellen kun er tilpasset til træningsdata.

## Krydsvalidering

Der er et problem med tilgangen ovenfor. Når man træner en model, er det
altid en fordel at have så meget data som muligt, da man så har mulighed
for at træne modellen meget præcist. Problemet er, at hvis man bruger
det meste af data som træningsdata, er der ikke meget tilbage til at
teste på, og vi risikerer overfitting.

Krydsvalidering løser dette problem på snedig vis ved at gentage
trænings- og testproceduren flere gange. For at lave $k$-fold
krydsvalidering deler man data op i $k$ lige store dele. I første fold
træner man modellen på alt data undtagen den første del og bruger første
del som testdata. Det given en tabsfunktion $E_1(p)$. Dette gentages så
$k$ gange, hvor man i den $i$'te fold bruger den $i$'te del af data som
testdata og resten som træningsdata og får en tabsfunktion $E_i(p)$. Idéen er illustreret i @fig-krydsvalidering.

```{tikz k-fold}
#| echo: false
#| fig-cap: Illustration af idéen ved $5$-fold krydsvalidering.
#| label: fig-krydsvalidering
\begin{tikzpicture}
	\draw[fill,red!30] (0,0) -- (2,0) -- (2,0.5) -- (0,0.5) -- (0,0);
	\draw[fill, blue!30] (2,0) -- (10,0) -- (10,0.5) -- (2,0.5) -- (2,0);
	\node at (1,0.25) {test} ;
	\node at (6,0.25) {træning} ;
	\node at (-1,0.25) {Fold 1} ;
	
	\draw[fill,blue!30] (0,-1) -- (2,-1) -- (2,-0.5) -- (0,-0.5) -- (0,-1);
	\draw[fill, red!30] (2,-1) -- (4,-1) -- (4,-0.5) -- (2,-0.5) -- (2,-1);
	\draw[fill, blue!30] (4,-1) -- (10,-1) -- (10,-0.5) -- (4,-0.5) -- (4,-1);
	\node at (3,-0.75) {test} ;
	\node at (7,-0.75) {træning} ;
	\node at (-1,-0.75) {Fold 2} ;
	
	\draw[fill,blue!30] (0,-2) -- (4,-2) -- (4,-1.5) -- (0,-1.5) -- (0,-2);
	\draw[fill, red!30] (4,-2) -- (6,-2) -- (6,-1.5) -- (4,-1.5) -- (4,-2);
	\draw[fill, blue!30] (6,-2) -- (10,-2) -- (10,-1.5) -- (6,-1.5) -- (6,-2);
	\node at (5,-1.75) {test} ;
	\node at (8,-1.75) {træning} ;
	\node at (-1,-1.75) {Fold 3} ;
	
	\draw[fill,blue!30] (0,-3) -- (6,-3) -- (6,-2.5) -- (0,-2.5) -- (0,-3);
	\draw[fill, red!30] (6,-3) -- (8,-3) -- (8,-2.5) -- (6,-2.5) -- (6,-3);
	\draw[fill, blue!30] (8,-3) -- (10,-3) -- (10,-2.5) -- (8,-2.5) -- (8,-3);
	\node at (7,-2.75) {test} ;
	\node at (3,-2.75) {træning} ;
	\node at (-1,-2.75) {Fold 4} ;
	
	\draw[fill,blue!30] (0,-4) -- (8,-4) -- (8,-3.5) -- (0,-3.5) -- (0,-4);
	\draw[fill, red!30] (8,-4) -- (10,-4) -- (10,-3.5) -- (8,-3.5) -- (8,-4);
	\node at (9,-3.75) {test} ;
	\node at (4,-3.75) {træning} ;
	\node at (-1,-3.75) {Fold 5} ;
\end{tikzpicture}
```

Som et samlet mål for, hvor god modellen er, bruges summen af
tabsfunktionerne fra de $k$ fold
$$E(p)=E_1(p) + E_2(p) + \dotsm + E_k(p).$$ Man vælger så den model, der
giver den mindste værdi af $E(p)$.

Fordelen ved krydsvalidering er, at man i hver fold bruger det meste af
data til at træne modellen på. Samtidig bliver hvert datapunkt alt i alt
brugt præcis én gang til at teste på. På den måde får man udnyttet data
bedre end, hvis man bare laver en enkelt opdeling af data. Typisk vælger
man $k=5$ eller $k=10$.



### Krydsvalidering i andre sammenhænge

Krydsvalidering kan bruges i et væld af andre sammenhænge, hvor der skal
vælges mellem flere forskellige prædiktionsmodeller.

Hvis det, der skal prædikteres, er en talværdi, kan man gøre som ovenfor.
Algoritmen trænes på træningsdataet og bruges derefter til lave
prædiktioner $\hat{y}_i^{test}$, $i=1,\ldots ,m$, af værdierne i
testdatasættet. Disse sammenlignes med de faktiske værdier $y_i^{test}$
ved at se på forskellene $y_i^{test} - \hat{y}_i^{test}$ og beregne
tabsfunktionen
$$E^{test} = \sum_{i=1}^m(y_i^{test}-\hat{y}_i^{test})^2.$$ Modellen med
lavest $E^{test}$ er bedst til at lave nye prædiktioner.

Hvis der derimod er tale om et klassifikationsproblem, hvor der skal
prædikteres en klasse (fx mand/kvinde, rød blok/blå blok, almindelig mail/spam), skal man definere tabsfunktionen lidt anderledes.
Som før trænes algoritmen på træningsdataet og derefter bruges den til
lave prædiktioner af klasserne $\hat{y}_i^{test}$, $i=1,\ldots ,m$, i
testdatasættet. Disse sammenholdes med de faktiske klasser, og vi bruger
så *fejlraten* som tabsfuntion. Fejlraten angiver andelen af
observationerne i testdataet, der bliver klassificeret forkert, det vil sige

$$
E^{test} =  \frac{1}{m}\cdot (\text{antal fejlklassifikationer i testdata}).
$$ {#eq-fejl}

Modellen med lavest $E^{test}$ har færrest fejlklassifikationer og
vælges derfor som den bedste.

Eksempler her fra siden, hvor krydsvalidering kan benyttes:

-   I forløbet [Hvem ligner du mest](../undervisningsforloeb/hvem_ligner_du_mest.qmd) sammenligner man et gråt punkt
    med alle punkter inden for en radius $r$ for at forudsige farven.
    Det er ikke oplagt, hvordan man skal vælge denne radius. En mulighed er
    at komme med nogle gode bud $r_1,\ldots, r_N$ på radier og så bruge
    krydsvalidering til at vælge den bedste. Én mulighed er at vælge
    $k=n$. Så får man det, der kaldes *leave-one-out* krydsvalidering. I
    hver fold bliver der så kun et datapunkt i testdata, mens resten
    bruges som træningsdata. Det ene testdatapunkt farves gråt, og
    farven prædikteres ud fra de øvrige datapunkter, der ligger inden for
    den valgte radius. Prædiktionen sammenlignes med punktets rigtige
    farve. Dette gentages for alle datapunkter og fejlraten (@eq-fejl)
    beregnes til sidst. Vi vælger så den radius, der har lavest fejlrate.
    
- I noten om [Kunstige neurale nerværk](neurale_net/neurale_net.qmd) beskrives det, hvordan man træner et kunstigt neuralt netværk. Men når man gør det, skal man på forhånd have besluttet sig for, hvor mange skjulte lag der skal være i netværket, og hvor mange neuroner, der skal være i hvert skjult lag. Jo flere skjulte lag og jo flere neuroner -- desto større fleksibilitet. Her vil krydsvalidering være oplagt til at afgøre, hvor *fleksibelt* netværket skal være samtidig med, at man undgår *overfitting*.
    
    
```{r unused-code}
#| include: false
## Kode der ikke bruges lige nu

# RSS test
#RSS2<-sum((x1+v -predict(model,data.frame(x=x)))^2)
#RSS3<-sum((x1+v -predict(model3,data.frame(x=x)))^2)
#RSS7<-sum((x1+v -predict(model7,data.frame(x=x)))^2)
#RSS træn
#RSS2<-sum((y-predict(model,data.frame(x=x)))^2)
#RSS3<-sum((y-predict(model3,data.frame(x=x)))^2)
#RSS7<-sum((y -predict(model7,data.frame(x=x)))^2)

# par(mfrow = c(1,2))
# model15<-lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7)
#             +I(x^8) + I(x^9) + I(x^10)+ I(x^11) + I(x^12))
# plot(x,y)
# lines(x2,predict(model,data.frame(x=x2)),col="red")
# lines(x2,predict(model15,data.frame(x=x2)),col="blue")
# plot(x,x1+v,col="green")
# lines(x2,predict(model,data.frame(x=x2)),col="red")
# lines(x2,predict(model15,data.frame(x=x2)),col="blue")

```

[^1]: Du er vant til, at forskriften for en lineær funktion er på formen $f(x)=ax+b$. Men lige om lidt viser skrivemåden $f(x)=a_0+a_1x$ sig nyttig. I forhold til det, du kender, svarer det til, at $a_0=b$ og $a_1=a$.
[^2]: I gymnasiet skriver vi som regel forskriften for et andengradspolynomium på formen $f(x)=ax^2+bx+c$. Med notationen, som vi bruger her, svarer det til, at $a_0=c, a_1=b$ og $a_2=a$.